{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e6712a-a347-49e0-90bc-7b70e04c7a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Local Retreival Augmented Generation\n",
    "## Trevor Andrus\n",
    "\n",
    "### The following notebook constitutes the functions and pacakges necessary to run two versions of RAG on a local machine: \n",
    "### 1. Pull context from Wikipedia \n",
    "### 2. Vectorize a local directory for context\n",
    "\n",
    "#### There are also 3 examples at the bottom of the notebook that show initial success in these techniques. \n",
    "#### The specifics of each function are detailed in the declarations below, but the general flow of each method is the following:\n",
    "\n",
    "\n",
    "## Wikipedia\n",
    "1. Take a user query\n",
    "2. Extract nouns, pronouns and named entities from query\n",
    "3. Dedupe these keywords\n",
    "4. Create all permuatations of these keywords\n",
    "5. For each keyword, pull the top N wiki articles\n",
    "6. Extract context from wiki articles sentence by sentence (from either summary or full text)\n",
    "7. Record the line of each sentence, article it came from, and url\n",
    "8. Create vector embedding of each sentence\n",
    "9. Push Vector, sentence, url, and title to a local pg vector database\n",
    "10. Taking the original query, pull the top n similar sentences as context from the database\n",
    "11. Return the resulting context as a dataframe\n",
    "12. Using the the context dataframe and original query, prompt generation model for final results\n",
    "\n",
    "\n",
    "## Local Directory\n",
    "1. Pass a local filepath to the directory you'd like to vectorize\n",
    "2. Function will loop through directory and identify 3 file types: docx, .ipyn, pdf\n",
    "2. (This can be expanded to accept additional file types)\n",
    "3. Extract context from each document line by line\n",
    "4. record line, document name, filepath and sentence\n",
    "5. vectorize sentence\n",
    "6. Push Vector, sentence, filepath, and title to a local pg vector database\n",
    "7. Taking the original query, pull the top n similar sentences as context from the database\n",
    "8. Return the resulting context as a dataframe\n",
    "9. Using the the context dataframe and original query, prompt generation model for final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e795b4be-7224-44f7-a59f-2c342fb7b9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-tandrus/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# wiki imports\n",
    "import wikipedia as wp\n",
    "from wikipedia import WikipediaPage\n",
    "\n",
    "# generation model and sentence transformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# NLP packages\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "\n",
    "# vector database\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "\n",
    "# for document ingestion\n",
    "import PyPDF2\n",
    "import nbformat\n",
    "from docx import Document\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "os.environ[\"HF_TOKEN\"]=\"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\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e299ec-388c-43bc-93d9-91e23197f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(sentence):\n",
    "    \"\"\"\n",
    "    Take an input sentence (usually the user's question), extract all nouns, pronouns and named entities\n",
    "    \n",
    "    PARAMETERS:\n",
    "        sentence: string representing the sentence\n",
    "        \n",
    "    RETURNS:\n",
    "        list of strings, each being element a noun or pronoun from the input\n",
    "    \"\"\"\n",
    "    \n",
    "    # use spacy to wrap the input sentence\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract keywords (nouns and proper nouns)\n",
    "    keywords = [token.text for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "    # return list of extracted elements\n",
    "    return list(set(keywords).union(set(entities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ff1fa6-023a-4ab6-afff-83c9865b480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedupe(keywords):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take the output of the extract_keywords functionn, and remove duplicates\n",
    "    (Also remove 'george bush' if 'george' and 'bush' exist)\n",
    "    \n",
    "    PARAMETERS:\n",
    "        keywords: list of keywords from the extract_keywords functionn\n",
    "    \n",
    "    RETURNS:\n",
    "        list of deduped keywords\n",
    "    \"\"\"\n",
    "    \n",
    "    words_to_delete = []\n",
    "    words = keywords.copy()\n",
    "    for i in range(len(words)):\n",
    "        if len(words[i].split()) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            if words[i].split()[0] in words and words[i].split()[1] in words:\n",
    "                words_to_delete.append(words[i].split()[0])\n",
    "                words_to_delete.append(words[i].split()[1])\n",
    "\n",
    "    for i in words_to_delete:\n",
    "        words.remove(i)\n",
    "        \n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3542ccb-506c-4e79-aac5-9e7c651482ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_list(user_query):\n",
    "    \"\"\"\n",
    "    Take the original user query, and call the two above functions\n",
    "    with the deduped output, create combinations of all keywords\n",
    "    if we have 'cat' and 'dog' and 'run', we return all permutations of the 3\n",
    "    \n",
    "    PARAMETERS:\n",
    "        user_query: string of raw user input\n",
    "        \n",
    "    RETURNS:\n",
    "        list of augmented keywords - all permutations of the deduped list\n",
    "    \"\"\"\n",
    "    \n",
    "    # get keywords\n",
    "    words = extract_keywords(user_query)\n",
    "    \n",
    "    # dedupe\n",
    "    words = dedupe(words)\n",
    "\n",
    "    new_words = words.copy()\n",
    "\n",
    "    # create all permuations of existing keywords\n",
    "    for i in words:\n",
    "        for j in words:\n",
    "            if i == j:\n",
    "                continue\n",
    "            new_word = str(i + ' ' + j)\n",
    "            if new_word in new_words:\n",
    "                continue\n",
    "            else:\n",
    "                new_words.append(new_word)\n",
    "                \n",
    "    # return new list\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec758a0-1a4a-4bc1-88bf-da4243ce0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_context(keywords, paragraph = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take the dedupded keywords from the input query, and gather wiki context for each\n",
    "    For each word, pull the top n wiki pages from search\n",
    "    Pull the content line by line (or paragraph by paragraph)\n",
    "    \n",
    "    PARAMETERS:\n",
    "        keywords: list of deduped keywords\n",
    "        paragraph (bool): if true, gather context in paragraphs, if false, by sentence\n",
    "        \n",
    "    RETURNS:\n",
    "        context_list: a list of strings, each element is the context pulled from wikipedia\n",
    "            (either a sentence or a paragraph)\n",
    "        file_names: a list of strings reprenting the wiki articles the context was retreived from\n",
    "        words: the words used to gather context\n",
    "        links: the links from wiki pages\n",
    "    \"\"\"\n",
    "    \n",
    "    # lists to store information gathered, the file it came from, and the word that produced it\n",
    "    context_list = []\n",
    "    gathered_files = []\n",
    "    file_names = []\n",
    "    words = []\n",
    "    links = []\n",
    "\n",
    "    # loop through all keywords\n",
    "    for word in keywords:\n",
    "        \n",
    "        # for each keyword, pull the top 2 wiki pages (change this if needed)\n",
    "        result = wp.search(word, results = 2)\n",
    "        \n",
    "        # for each wiki page\n",
    "        for i in result:\n",
    "            try:\n",
    "                \n",
    "                # if we've already stored the info from this page, skip\n",
    "                page = wp.page(i, auto_suggest=False)\n",
    "                if page.title in gathered_files:\n",
    "                    continue\n",
    "                \n",
    "                # record the name of the page, which word was used to search\n",
    "                # and the url of the page\n",
    "                file_names.append(page.title)\n",
    "                gathered_files.append(page.title)\n",
    "                words.append(word)\n",
    "                links.append(page.url)\n",
    "                \n",
    "                # get the summary of the page, and split it into sentences\n",
    "                context = page.summary # can also change this to pull the full page if needed\n",
    "                sentences = sent_tokenize(context)\n",
    "                sentences = list(OrderedDict.fromkeys(sentences))\n",
    "                \n",
    "                # group sentences into paragraphs (looking for \\n is unreliable, so these \n",
    "                # are psuedo-paragraphs of sentences)\n",
    "                if paragraph:\n",
    "                    grouped_list = [''.join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
    "                    sentences = grouped_list\n",
    "                \n",
    "                \n",
    "                # we gather all this to create a dataframe as output\n",
    "                context_list.append(sentences)\n",
    "                file_names.append(file)\n",
    "                key_words.append(current_word)\n",
    "                \n",
    "            # we pull the name of the wiki article, then grab it by title\n",
    "            # sometimes the title returned in a search is not directly associated with a wiki page\n",
    "            # if this is the case, an error is thrown. We use this to skip these cases. \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    return context_list, file_names, words, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb40f55-8e2f-457f-9e20-3e662d7f82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_context(context_list, file_names, key_words, links):\n",
    "    \"\"\"\n",
    "    Using the output of gather_context, create a database table\n",
    "    each row in the table contains:\n",
    "        vectorized context, context, file name, and link\n",
    "        \n",
    "    PARAMETERS:\n",
    "        context_list: a list of strings, each element is the context pulled from wikipedia\n",
    "            (either a sentence or a paragraph)\n",
    "        file_names: a list of strings reprenting the wiki articles the context was retreived from\n",
    "        words: the words used to gather context\n",
    "        links: the links from wiki pages\n",
    "    \"\"\"\n",
    "\n",
    "    # create connection\n",
    "    conn = psycopg.connect(dbname='rag', user='rag', password='rag', autocommit=True)\n",
    "    conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "    register_vector(conn)\n",
    "\n",
    "    # replace old data\n",
    "    conn.execute('DROP TABLE IF EXISTS documents')\n",
    "    conn.execute('CREATE TABLE documents (id bigserial PRIMARY KEY, line integer, word text, document text, content text, embedding vector(384), link text, page text)')\n",
    "    \n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    for i in range(len(context_list)):\n",
    "        doc_name = []\n",
    "        line = []\n",
    "        words = []\n",
    "        link = []\n",
    "        page = []\n",
    "        for j in range(len(context_list[i])):\n",
    "            doc_name.append(file_names[i])\n",
    "            line.append(j)\n",
    "            page.append('N/A')\n",
    "            words.append(key_words[i])\n",
    "            link.append(links[i])\n",
    "        embeddings = model.encode(context_list[i])\n",
    "\n",
    "        for content, embedding, word, doc_name, line, link, page in zip(context_list[i], embeddings, words, doc_name, line, link, page):\n",
    "            conn.execute('INSERT INTO documents (line, word, document, content, embedding, link, page) VALUES (%s, %s, %s, %s, %s, %s, %s)', (line, word, doc_name, content, embedding, link, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59219eef-2c89-46f1-b532-7664681afcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_database(query, n):\n",
    "    \"\"\"\n",
    "    Query the local vector database to the top n cosine similar vectors to the input\n",
    "    \n",
    "    PARAMETERS:\n",
    "        query: string to be matched in database\n",
    "        n: number of results to be returned\n",
    "        \n",
    "    RETURNS:\n",
    "        pandas dataframe representing query results\n",
    "    \n",
    "    \"\"\"\n",
    "    string = query\n",
    "    \n",
    "    # instantiate sentence transformer for embedding\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # embed the input string\n",
    "    embedding = model.encode(string)\n",
    "    \n",
    "    # establish connection to database\n",
    "    conn = psycopg.connect(dbname='rag', user='rag', password='rag', autocommit=True)\n",
    "\n",
    "    # create embedding array for pgvector\n",
    "    embedding_pgarray = \"[\" + \",\".join(map(str, embedding)) + \"]\"\n",
    "\n",
    "    # Execute a SQL query to find the top 5 vectors closest to the embedding vector\n",
    "    query = \"\"\"\n",
    "        SELECT content, document, page, line, link\n",
    "        FROM documents \n",
    "        ORDER BY embedding <=> %(embedding)s \n",
    "        LIMIT %(n)s\n",
    "    \"\"\"\n",
    "    neighbors = conn.execute(query, {'embedding': embedding_pgarray, 'n':n}).fetchall()\n",
    "    \n",
    "    # organize query results into dataframe\n",
    "    df = pd.DataFrame()\n",
    "    for neighbor in neighbors:\n",
    "        content = neighbor[0].strip()\n",
    "        document = neighbor[1]\n",
    "        page = neighbor[2]\n",
    "        line = neighbor[3]\n",
    "        link = neighbor[4]\n",
    "\n",
    "        df1 = pd.DataFrame({'Content':content, 'Document':document,'Page':page, 'Line':line, 'Link':link}, index=[0])\n",
    "        df = pd.concat((df, df1))\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d67452-77de-4571-9afb-4ed9abcce3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def internet_query(query, n, paragraph=False):\n",
    "    \"\"\"\n",
    "    Combine all the above functions into a single call - take a query and return results from wikipedia\n",
    "    \n",
    "    PARAMETERS:\n",
    "        query: raw string input from user\n",
    "        n: number of results to return:\n",
    "        paragraph (bool): whether or not to return context as paragraphs\n",
    "        \n",
    "    RETURNS:\n",
    "        dataframe with query results\n",
    "    \"\"\"\n",
    "    # create search terms\n",
    "    new_words = create_word_list(query)\n",
    "    \n",
    "    # gather context from search terms\n",
    "    context_list, file_names, key_words, links = gather_context(new_words, paragraph)\n",
    "    \n",
    "    # vectorize gathered context\n",
    "    vectorize_context(context_list, file_names, key_words, links)\n",
    "    \n",
    "    # return top n results\n",
    "    return query_database(query, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3307df29-74eb-4660-8032-edbdd2fa65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path, paragraph = False):\n",
    "    \"\"\"\n",
    "    Function to import pdf content and split it into sentences\n",
    "    \n",
    "    PARAMETERS:\n",
    "        pdf_path: path to pdf file\n",
    "        paragraph (bool): whether or not to process as sentences\n",
    "        \n",
    "    RETURNS:\n",
    "        list of gathered sentences from pdf's\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    \n",
    "    full_sentences = []\n",
    "    # get all lines from pdf file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        num_pages = len(reader.pages)\n",
    "        page_numbers = []\n",
    "        for page_num in range(num_pages):\n",
    "            # add page number to db          \n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            page_text = sent_tokenize(text)\n",
    "            page_text = list(OrderedDict.fromkeys(page_text))\n",
    "            \n",
    "            for i in range(len(page_text)):\n",
    "                page_numbers.append(page_num)\n",
    "                           \n",
    "            full_sentences.extend(page_text)\n",
    "    \n",
    "    # group if necessary\n",
    "    if paragraph:\n",
    "        grouped_list = [''.join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
    "        sentences = grouped_list\n",
    "    \n",
    "    return full_sentences, page_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20a00eb-ebbc-499f-8c5a-9924e4377305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_notebook(notebook_file_path, paragraph=False):\n",
    "    \"\"\"\n",
    "     Function to import ipynb content and split it into sentences\n",
    "    \n",
    "    PARAMETERS:\n",
    "        pdf_path: path to ipynb file\n",
    "        paragraph (bool): whether or not to process as sentences\n",
    "        \n",
    "    RETURNS:\n",
    "        list of gathered sentences from ipynb's\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the notebook file\n",
    "    with open(notebook_file_path, 'r', encoding='utf-8') as f:\n",
    "        notebook_content = nbformat.read(f, as_version=4)\n",
    "\n",
    "    # Extract the cells from the notebook content\n",
    "    cells = notebook_content['cells']\n",
    "\n",
    "    # Initialize an empty list to store the lines\n",
    "    lines = []\n",
    "\n",
    "    # Iterate through cells and extract text from code and markdown cells\n",
    "    for cell in cells:\n",
    "        if cell['cell_type'] == 'code':\n",
    "            # Extract code cell content and split by newline characters\n",
    "            lines.extend(cell['source'].splitlines())\n",
    "        elif cell['cell_type'] == 'markdown':\n",
    "            # Extract markdown cell content and split by newline characters\n",
    "            lines.extend(cell['source'].splitlines())\n",
    "          \n",
    "    # group if necessary\n",
    "    if paragraph:\n",
    "        grouped_list = [''.join(lines[i:i+4]) for i in range(0, len(lines), 4)]\n",
    "        lines = grouped_list\n",
    "\n",
    "    # Return the list of lines\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d98bd8-6168-4c2a-9def-af67bea06ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_doc(word_doc_file_path, paragraph=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to import docx content and split it into sentences\n",
    "    \n",
    "    PARAMETERS:\n",
    "        pdf_path: path to docx file\n",
    "        paragraph (bool): whether or not to process as sentences\n",
    "        \n",
    "    RETURNS:\n",
    "        list of gathered sentences from docx's\n",
    "    \"\"\"\n",
    "    \n",
    "    # create word doc object\n",
    "    doc = Document(word_doc_file_path)\n",
    "\n",
    "    # Initialize an empty string to store the extracted text\n",
    "    text = \"\"\n",
    "\n",
    "    # Iterate through paragraphs in the document and extract text\n",
    "    for paragraph1 in doc.paragraphs:\n",
    "        text += paragraph1.text + \"\\n\"\n",
    "\n",
    "    # break text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = list(OrderedDict.fromkeys(sentences))\n",
    "    \n",
    "    # group if necessary\n",
    "    if paragraph:\n",
    "        grouped_list = [''.join(sentences[i:i+4]) for i in range(0, len(sentences), 4)]\n",
    "        sentences = grouped_list\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf404b1c-e689-46d9-8fe6-ba833f739870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_directory(directory, paragraph=False):\n",
    "    \"\"\"\n",
    "    Loop through a given directory, create a large list of sentences from all compatible files\n",
    "    Push large list of sentences to vector database\n",
    "    \n",
    "    PARAMETERS:\n",
    "        directory: file path to directory to be vectorized\n",
    "        paragraph: whether to group context in paragraphs\n",
    "    \"\"\"\n",
    "    \n",
    "    # instantiate sentences transformer for embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # create connection\n",
    "    conn = psycopg.connect(dbname='rag', user='rag', password='rag', autocommit=True)\n",
    "    conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "    register_vector(conn)\n",
    "\n",
    "    # replace old data\n",
    "    conn.execute('DROP TABLE IF EXISTS documents')\n",
    "    conn.execute('CREATE TABLE documents (id bigserial PRIMARY KEY, line integer, document text, content text, embedding vector(384), link text, page text)')\n",
    "\n",
    "    # loop through directory\n",
    "    directory_path = directory\n",
    "    all_lines = []\n",
    "    file_names = []\n",
    "    links = []\n",
    "    pages_by_doc = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.ipynb'):\n",
    "            \n",
    "            # Construct the full path to the notebook file\n",
    "            notebook_file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Process the notebook file and append lines to the list\n",
    "            lines = process_notebook(notebook_file_path, paragraph)\n",
    "            file_names.append(filename)\n",
    "            \n",
    "            pages = []\n",
    "            for i in range(len(lines)):\n",
    "                pages.append('N/A')\n",
    "            \n",
    "            pages_by_doc.append(pages)\n",
    "                \n",
    "            all_lines.append(lines)\n",
    "            links.append('/user/tandrus/lab/tree/' + file_path[2:])\n",
    "            \n",
    "        elif filename.endswith('.docx'):\n",
    "            \n",
    "            # Construct the full path to the word doc file\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            file_names.append(filename)\n",
    "\n",
    "            sentences = process_word_doc(file_path, paragraph)\n",
    "            \n",
    "            pages = []\n",
    "            for i in range(len(sentences)):\n",
    "                pages.append('N/A')\n",
    "            \n",
    "            pages_by_doc.append(pages)\n",
    "                \n",
    "            all_lines.append(sentences)\n",
    "            links.append('/user/tandrus/lab/tree/' + file_path[2:])\n",
    "            \n",
    "        elif filename.endswith('.pdf'):\n",
    "            \n",
    "            # Construct the full path to the pdf file\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            file_names.append(filename)\n",
    "\n",
    "            sentences, pdf_pages = process_pdf(file_path, paragraph)\n",
    "            \n",
    "            pages_by_doc.append(pdf_pages)\n",
    "            \n",
    "            all_lines.append(sentences)\n",
    "            links.append('/user/tandrus/lab/tree/' + file_path[2:])\n",
    "\n",
    "    print('Number of Documents: ', len(all_lines))\n",
    "    \n",
    "    # push all gathered context to vector database\n",
    "    for i in range(len(all_lines)):\n",
    "        \n",
    "        print(file_names[i], ': ', len(all_lines[i]), ' sentences found.')\n",
    "        doc_name = []\n",
    "        line = []\n",
    "        link = []\n",
    "        page = []\n",
    "        for j in range(len(all_lines[i])):\n",
    "            doc_name.append(file_names[i])\n",
    "            line.append(j)\n",
    "            link.append(links[i])\n",
    "            page.append(pages_by_doc[i][j])\n",
    "        embeddings = model.encode(all_lines[i])\n",
    "\n",
    "        for content, embedding, doc_name, line, link, page in zip(all_lines[i], embeddings, doc_name, line, link, page):\n",
    "            conn.execute('INSERT INTO documents (line, document, content, embedding, link, page) VALUES (%s, %s, %s, %s, %s, %s)', (line, doc_name, content, embedding, link, page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43113df-9f9f-4785-9f2d-e734b9bb7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_long_context(context_df, length):\n",
    "    \"\"\"\n",
    "    Take the results of a query (dataframe), append all the context to a single strinng\n",
    "    limit the context to a specified length\n",
    "    \n",
    "    PARAMTERS:\n",
    "        context_df: pandas dataframe output from a vector db query\n",
    "        length: desired length of long context\n",
    "    \"\"\"\n",
    "    \n",
    "    # append all rows from context df 'content' column\n",
    "    context = ''\n",
    "    for i in range(len(context_df)):\n",
    "        context += context_df.loc[i, 'Content']\n",
    "\n",
    "    context = context[:length]\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e327b7-1f03-4baf-b97f-858a8c8e1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(query, result_df, local=False):   \n",
    "    \"\"\"\n",
    "    Prompt engineering for generation\n",
    "    Combine query and context\n",
    "     \n",
    "    PARAMETERS:\n",
    "        query: string query from user\n",
    "        result_df: pandas dataframe from query result\n",
    "    \n",
    "    \"\"\"\n",
    "    context = create_long_context(result_df, 1000)\n",
    "    prompt = f\"\"\"\n",
    "    Context: {context}\n",
    "\n",
    "    Question: According to the context provided, {query}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d40dcf-b674-4bc5-bd27-02c149c131ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, query):\n",
    "    \"\"\"\n",
    "    Given a prompt, use google's gemma 7b to generate output\n",
    "    \n",
    "    PARAMETERS:\n",
    "        prompt: prompt for generation model\n",
    "        query: original query used to create the prompt\n",
    "    \"\"\"\n",
    "    # define output length\n",
    "    max_length = 500\n",
    "    \n",
    "    # tokenize input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length)\n",
    "    \n",
    "    # generate\n",
    "    outputs = model.generate(**input_ids, max_length=max_length)\n",
    "    \n",
    "    # gather response\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    # if the output contains 'cannot answer' - (the answer to the question is not found in the context)\n",
    "    # regenerate without context. \n",
    "    if 'cannot answer' in response:\n",
    "        print('Context was unhelpful - regenerating')\n",
    "        input_ids = tokenizer(query, return_tensors=\"pt\", max_length=max_length)\n",
    "        outputs = model.generate(**input_ids, max_length=max_length)\n",
    "        response = tokenizer.decode(outputs[0])\n",
    "        print(response)\n",
    "    else:\n",
    "        \n",
    "        # I split the response because the generation tends to include the full question and context\n",
    "        # in its output, this removes that for cleaner output\n",
    "        print(response.split('Question')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8f8b087-2bb4-4a41-81ff-532e7533d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clickable(val):\n",
    "    \"\"\"\n",
    "    Function to make links clickable in the result dataframes\n",
    "    \"\"\"\n",
    "    # target _blank to open new window\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d1890-7cce-4190-9370-06a04a916b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dbe62d1d3a46fe8343bfc867b051eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afc675aa0d54d90a592f1a1b48f067c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86633e7b8fc4d728127e5f4c96e1971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4596a903dc994788aa9b87630ef6ce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1144cedba4646d58939c4ed2ee947d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c6f89-215f-4d04-8cd5-416b93372a85",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba924a5-e4d4-48f2-a5b4-0a0b36ebc5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'was george bush president in 2008?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cb348-d7d0-492f-9243-b647a7219fea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.25 s, sys: 106 ms, total: 6.35 s\n",
      "Wall time: 9.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_abcaf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_abcaf_level0_col0\" class=\"col_heading level0 col0\" >Content</th>\n",
       "      <th id=\"T_abcaf_level0_col1\" class=\"col_heading level0 col1\" >Document</th>\n",
       "      <th id=\"T_abcaf_level0_col2\" class=\"col_heading level0 col2\" >Page</th>\n",
       "      <th id=\"T_abcaf_level0_col3\" class=\"col_heading level0 col3\" >Line</th>\n",
       "      <th id=\"T_abcaf_level0_col4\" class=\"col_heading level0 col4\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_abcaf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_abcaf_row0_col0\" class=\"data row0 col0\" >George Walker Bush (born July 6, 1946) is an American politician and businessman who served as the 43rd president of the United States from 2001 to 2009.</td>\n",
       "      <td id=\"T_abcaf_row0_col1\" class=\"data row0 col1\" >George W. Bush</td>\n",
       "      <td id=\"T_abcaf_row0_col2\" class=\"data row0 col2\" >N/A</td>\n",
       "      <td id=\"T_abcaf_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_abcaf_row0_col4\" class=\"data row0 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/George_W._Bush\">https://en.wikipedia.org/wiki/George_W._Bush</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_abcaf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_abcaf_row1_col0\" class=\"data row1 col0\" >The 2008 United States presidential election was the 56th quadrennial presidential election, held on November 4, 2008.</td>\n",
       "      <td id=\"T_abcaf_row1_col1\" class=\"data row1 col1\" >2008 United States presidential election</td>\n",
       "      <td id=\"T_abcaf_row1_col2\" class=\"data row1 col2\" >N/A</td>\n",
       "      <td id=\"T_abcaf_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_abcaf_row1_col4\" class=\"data row1 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/2008_United_States_presidential_election\">https://en.wikipedia.org/wiki/2008_United_States_presidential_election</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_abcaf_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_abcaf_row2_col0\" class=\"data row2 col0\" >George Herbert Walker Bush (June 12, 1924 – November 30, 2018) was an American politician, diplomat, and businessman who served as the 41st president of the United States from 1989 to 1993.</td>\n",
       "      <td id=\"T_abcaf_row2_col1\" class=\"data row2 col1\" >George H. W. Bush</td>\n",
       "      <td id=\"T_abcaf_row2_col2\" class=\"data row2 col2\" >N/A</td>\n",
       "      <td id=\"T_abcaf_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_abcaf_row2_col4\" class=\"data row2 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/George_H._W._Bush\">https://en.wikipedia.org/wiki/George_H._W._Bush</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fec00bdc4c0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "query = 'was george bush president in 2008'\n",
    "result_df = internet_query(query, 10, paragraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e21a9c8-026e-48f9-b7db-b5406bfcced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": According to the context provided, was george bush president in 2008\n",
      "    Answer: Yes, George Bush was president in 2008.<eos>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5a97d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5a97d_level0_col0\" class=\"col_heading level0 col0\" >Content</th>\n",
       "      <th id=\"T_5a97d_level0_col1\" class=\"col_heading level0 col1\" >Document</th>\n",
       "      <th id=\"T_5a97d_level0_col2\" class=\"col_heading level0 col2\" >Page</th>\n",
       "      <th id=\"T_5a97d_level0_col3\" class=\"col_heading level0 col3\" >Line</th>\n",
       "      <th id=\"T_5a97d_level0_col4\" class=\"col_heading level0 col4\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5a97d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5a97d_row0_col0\" class=\"data row0 col0\" >George Walker Bush (born July 6, 1946) is an American politician and businessman who served as the 43rd president of the United States from 2001 to 2009.</td>\n",
       "      <td id=\"T_5a97d_row0_col1\" class=\"data row0 col1\" >George W. Bush</td>\n",
       "      <td id=\"T_5a97d_row0_col2\" class=\"data row0 col2\" >N/A</td>\n",
       "      <td id=\"T_5a97d_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_5a97d_row0_col4\" class=\"data row0 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/George_W._Bush\">https://en.wikipedia.org/wiki/George_W._Bush</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a97d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5a97d_row1_col0\" class=\"data row1 col0\" >The 2008 United States presidential election was the 56th quadrennial presidential election, held on November 4, 2008.</td>\n",
       "      <td id=\"T_5a97d_row1_col1\" class=\"data row1 col1\" >2008 United States presidential election</td>\n",
       "      <td id=\"T_5a97d_row1_col2\" class=\"data row1 col2\" >N/A</td>\n",
       "      <td id=\"T_5a97d_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_5a97d_row1_col4\" class=\"data row1 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/2008_United_States_presidential_election\">https://en.wikipedia.org/wiki/2008_United_States_presidential_election</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a97d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5a97d_row2_col0\" class=\"data row2 col0\" >George Herbert Walker Bush (June 12, 1924 – November 30, 2018) was an American politician, diplomat, and businessman who served as the 41st president of the United States from 1989 to 1993.</td>\n",
       "      <td id=\"T_5a97d_row2_col1\" class=\"data row2 col1\" >George H. W. Bush</td>\n",
       "      <td id=\"T_5a97d_row2_col2\" class=\"data row2 col2\" >N/A</td>\n",
       "      <td id=\"T_5a97d_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_5a97d_row2_col4\" class=\"data row2 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/George_H._W._Bush\">https://en.wikipedia.org/wiki/George_H._W._Bush</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7febde3b3ac0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = create_prompt(query, result_df)\n",
    "generate(prompt, query)\n",
    "print()\n",
    "pretty_df = result_df.head(3).style.format({'Link': make_clickable})\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970d67b-712c-480c-9302-04e066ea92bc",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "549c55b1-a075-401a-9713-2c5544f436ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.46 s, sys: 355 ms, total: 9.82 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = 'Was Steve young quarterback for byu in 1984'\n",
    "result_df = internet_query(query, 10, paragraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9869e869-e48d-472a-b03c-7cbb8df12ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context was unhelpful - regenerating\n",
      "<bos>Was Steve young quarterback for byu in 1984?\n",
      "\n",
      "The answer is no. Steve Young was not a quarterback for BYU in 1984.<eos>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_90ab8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_90ab8_level0_col0\" class=\"col_heading level0 col0\" >Content</th>\n",
       "      <th id=\"T_90ab8_level0_col1\" class=\"col_heading level0 col1\" >Document</th>\n",
       "      <th id=\"T_90ab8_level0_col2\" class=\"col_heading level0 col2\" >Page</th>\n",
       "      <th id=\"T_90ab8_level0_col3\" class=\"col_heading level0 col3\" >Line</th>\n",
       "      <th id=\"T_90ab8_level0_col4\" class=\"col_heading level0 col4\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_90ab8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_90ab8_row0_col0\" class=\"data row0 col0\" >Jon Steven Young (born October 11, 1961) is an American former football quarterback who played in the National Football League (NFL) for 15 seasons, most notably with the San Francisco 49ers.He was drafted by and played for the Tampa Bay Buccaneers.Prior to his NFL career, Young was a member of the Los Angeles Express in the United States Football League (USFL) for two seasons.He played college football for the BYU Cougars, setting school and NCAA records en route to being runner-up for the 1983 Heisman Trophy.</td>\n",
       "      <td id=\"T_90ab8_row0_col1\" class=\"data row0 col1\" >Steve Young</td>\n",
       "      <td id=\"T_90ab8_row0_col2\" class=\"data row0 col2\" >N/A</td>\n",
       "      <td id=\"T_90ab8_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_90ab8_row0_col4\" class=\"data row0 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Steve_Young\">https://en.wikipedia.org/wiki/Steve_Young</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90ab8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_90ab8_row1_col0\" class=\"data row1 col0\" >The 1984 BYU Cougars football team represented Brigham Young University (BYU) in the 1984 NCAA Division I-A football season.The Cougars were led by 13th-year head coach LaVell Edwards and played their home games at Cougar Stadium in Provo, Utah.The team competed as a member of the Western Athletic Conference, winning the conference for the ninth consecutive year.The Cougars finished the regular season as the only undefeated team in Division I-A, and secured their first ever national title by defeating Michigan in the 1984 Holiday Bowl.</td>\n",
       "      <td id=\"T_90ab8_row1_col1\" class=\"data row1 col1\" >1984 BYU Cougars football team</td>\n",
       "      <td id=\"T_90ab8_row1_col2\" class=\"data row1 col2\" >N/A</td>\n",
       "      <td id=\"T_90ab8_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_90ab8_row1_col4\" class=\"data row1 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/1984_BYU_Cougars_football_team\">https://en.wikipedia.org/wiki/1984_BYU_Cougars_football_team</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90ab8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_90ab8_row2_col0\" class=\"data row2 col0\" >The BYU Cougars football team is the college football program representing Brigham Young University (BYU) in Provo, Utah.The Cougars began collegiate football competition in 1922, and have won 23 conference championships and one national championship in 1984.The team has competed in several different athletic conferences during its history, from July 1, 2011 to 2022, they competed as an FBS Independent.On September 10, 2021, the Big 12 Conference unanimously accepted BYU’s application to the conference.</td>\n",
       "      <td id=\"T_90ab8_row2_col1\" class=\"data row2 col1\" >BYU Cougars football</td>\n",
       "      <td id=\"T_90ab8_row2_col2\" class=\"data row2 col2\" >N/A</td>\n",
       "      <td id=\"T_90ab8_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_90ab8_row2_col4\" class=\"data row2 col4\" ><a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/BYU_Cougars_football\">https://en.wikipedia.org/wiki/BYU_Cougars_football</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7febd9386520>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = create_prompt(query, result_df)\n",
    "generate(prompt, query)\n",
    "pretty_df = result_df.head(3).style.format({'Link': make_clickable})\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410baf6-07c9-427d-8de0-646ae551429c",
   "metadata": {},
   "source": [
    "# Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "781863e2-6cd0-4397-9d68-4c8c3c5b56e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents:  4\n",
      "Infant Health Study.pdf :  957  sentences found.\n",
      "FetalUltrasounds.pdf :  211  sentences found.\n",
      "Scheduling.ipynb :  1844  sentences found.\n",
      "Critical Analysis.docx :  51  sentences found.\n",
      "\n",
      "CPU times: user 1min 35s, sys: 4.32 s, total: 1min 40s\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorize_directory('./Examples', paragraph=False)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2a72e61c-6f3e-445d-8b6c-365093b12837",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Are ultrasounds important for prenatal care?'\n",
    "result_df = query_database(query, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b1e18bd-a691-4b15-b6ca-e1472d93e076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_068e7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_068e7_level0_col0\" class=\"col_heading level0 col0\" >Content</th>\n",
       "      <th id=\"T_068e7_level0_col1\" class=\"col_heading level0 col1\" >Document</th>\n",
       "      <th id=\"T_068e7_level0_col2\" class=\"col_heading level0 col2\" >Page</th>\n",
       "      <th id=\"T_068e7_level0_col3\" class=\"col_heading level0 col3\" >Line</th>\n",
       "      <th id=\"T_068e7_level0_col4\" class=\"col_heading level0 col4\" >Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_068e7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_068e7_row0_col0\" class=\"data row0 col0\" >B ACKGROUND\n",
       "When it comes to prenatal care, obtaining quality ultra-\n",
       "sounds and properly understanding those ultrasounds is critical\n",
       "to achieving proper diagnoses and providing premium patient\n",
       "care.</td>\n",
       "      <td id=\"T_068e7_row0_col1\" class=\"data row0 col1\" >FetalUltrasounds.pdf</td>\n",
       "      <td id=\"T_068e7_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_068e7_row0_col3\" class=\"data row0 col3\" >2</td>\n",
       "      <td id=\"T_068e7_row0_col4\" class=\"data row0 col4\" ><a target=\"_blank\" href=\"/user/tandrus/lab/tree/Examples/FetalUltrasounds.pdf\">/user/tandrus/lab/tree/Examples/FetalUltrasounds.pdf</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_068e7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_068e7_row1_col0\" class=\"data row1 col0\" >A LTERNATIVE DATA\n",
       "Due to the time constraints on the project, and the diffi-\n",
       "culty of obtaining the desired dataset, a premade alternative\n",
       "ultrasound dataset, concerned with detecting and diagnosing\n",
       "tumors in ultrasounds, was used in an effort to test potential\n",
       "architectures in preparation for when the fetal dataset would\n",
       "be ready for use.</td>\n",
       "      <td id=\"T_068e7_row1_col1\" class=\"data row1 col1\" >FetalUltrasounds.pdf</td>\n",
       "      <td id=\"T_068e7_row1_col2\" class=\"data row1 col2\" >2</td>\n",
       "      <td id=\"T_068e7_row1_col3\" class=\"data row1 col3\" >72</td>\n",
       "      <td id=\"T_068e7_row1_col4\" class=\"data row1 col4\" ><a target=\"_blank\" href=\"/user/tandrus/lab/tree/Examples/FetalUltrasounds.pdf\">/user/tandrus/lab/tree/Examples/FetalUltrasounds.pdf</a></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_068e7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_068e7_row2_col0\" class=\"data row2 col0\" >It is important to understand what healthcare profes -\n",
       "sionals who meet expecting and new parents through \n",
       "pregnancy, childbirth, and postnatal care follow-ups \n",
       "should consider when providing support and informa -\n",
       "tion.</td>\n",
       "      <td id=\"T_068e7_row2_col1\" class=\"data row2 col1\" >Infant Health Study.pdf</td>\n",
       "      <td id=\"T_068e7_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "      <td id=\"T_068e7_row2_col3\" class=\"data row2 col3\" >49</td>\n",
       "      <td id=\"T_068e7_row2_col4\" class=\"data row2 col4\" ><a target=\"_blank\" href=\"/user/tandrus/lab/tree/Examples/Infant Health Study.pdf\">/user/tandrus/lab/tree/Examples/Infant Health Study.pdf</a></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7febe3359880>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretty_df = result_df.head(3).style.format({'Link': make_clickable})\n",
    "\n",
    "pretty_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f9c997-0c3e-45c3-b10a-66e40c1a33e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Yes, according to the text, ultrasounds are important for prenatal care as they are used to achieve proper diagnoses and provide premium patient care.<eos>\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(query, result_df)\n",
    "generate(prompt, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8c147742-dbd1-4801-9dc4-4f6e5adbad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": According to the context provided, Are ultrasounds important for prenatal care?\n",
      "    Answer: Yes, according to the text, ultrasounds are important for prenatal care as they are used to achieve proper diagnoses and provide premium patient care.<eos>\n"
     ]
    }
   ],
   "source": [
    "prompt = create_prompt(query, result_df)\n",
    "generate(prompt, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b850128-d78b-4ec7-b942-5649a9af344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg.connect(dbname='rag', user='rag', password='rag', autocommit=True)\n",
    "                       \n",
    "# Execute a SQL query to find the top 5 vectors closest to the embedding vector\n",
    "query = \"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM documents \n",
    "\"\"\"\n",
    "neighbors = conn.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73955fa6-06f0-4b41-863b-56fa47a2a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(525,)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paragraph\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a207ddbe-2d3e-4090-ba75-93b018bf15a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2097,)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence by sentence\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e49b4-4db7-4710-bf46-d9d03928e2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
